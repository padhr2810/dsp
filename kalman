

import numpy as np

"""
PARAMETERS IN STEP 1:
X = mean state estimate of previous step (k-1)
P = state covariance of previous step (k-1)
A = transition matrix (nxn dimensions)
Q = process noise covariance matrix
B = input effect matrix
U = control input
"""

################################################ Step 1: Anterior probability -predict the mean 'X' and covariance 'P' at time step K.

def kf_predict(X, P, A, Q, B, U):
    X = np.dot(A, X) + np.dot(B, U)    
    P = np.dot(A, np.dot(P, A.T)) + Q     
    return(X,P)

	
"""
PARAMETERS IN STEP 2:
K = kalman gain matrix
IM = mean of predictive distribution of Y
IS = covariance or predictive mean of Y
LH = predictive probability (likelihood) of measurement
"""

################################################ Step 2: update step. Compute posterior mean 'X' and covariance matrix 'P' given a new measurement 'Y'.
def kf_update(X, P, Y, H, R):     
    IM = np.dot(H, X)     
    IS = R + np.dot(H, np.dot(P, H.T))     
    K = np.dot(P, np.dot(H.T, np.linalg.inv(IS)))     
    X = X + np.dot(K, (Y-IM))     
    P = P - np.dot(K, np.dot(IS, K.T))     
    LH = gauss_pdf(Y, IM, IS)     
    return (X,P,K,IM,IS,LH) 
 
def gauss_pdf(X, M, S):     
    if M.shape()[1] == 1:         
        DX = X - np.tile(M, X.shape()[1])           
        E = 0.5 * np.sum(DX * (np.dot(np.linalg.inv(S), DX)), axis=0)         
        E = E + 0.5 * M.shape()[0] * log(2 * pi) + 0.5 * log(det(S))         
        P = exp(-E)     
    elif X.shape()[1] == 1:         
        DX = np.tile(X, M.shape()[1])- M           
        E = 0.5 * np.sum(DX * (np.dot(np.linalg.inv(S), DX)), axis=0)         
        E = E + 0.5 * M.shape()[0] * log(2 * pi) + 0.5 * log(det(S))         
        P = exp(-E)     
    else:         DX = X-M           
        E = 0.5 * np.dot(DX.T, np.dot(np.linalg.inv(S), DX))         
        E = E + 0.5 * M.shape()[0] * log(2 * pi) + 0.5 * log(det(S))         
        P = exp(-E) 
 
    return (P[0],E[0])
    
    
    

    
    
    
